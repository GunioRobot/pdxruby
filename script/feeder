#!/usr/bin/env ruby

# = feeder
#
# purpose:: updates the Articles relation, aggregating
#   Members' feeds. Can be run as a daemon, or
#   just once.
#
# = Useful Links
#
#  * http://simple-rss.rubyforge.org/
#  * http://blogs.law.harvard.edu/tech/rss

$: << File.join(File.dirname(__FILE__), '../lib')

require 'optparse'
require 'rubygems'
require 'net/http'
require 'simple-rss'
require 'rexml/text'
require 'digest/md5'

OPTIONS = {
  :environment => "production",
  # set this to 0 if you only want it to run once
  # otherwise, set it to the number of seconds between polls
  :wait_time => 0,
  # set true if you prefer silence, false if you like verbosage
  :quiet => false
}

ARGV.options do |opts|
  opts.banner = "Usage: ruby #{File.basename($0)} [options]"
  
  opts.separator ''
  
  opts.on('-e', '--environment=name', String,
    'Specifies the environment to run this server under (test/development/production).',
    'Default: development') { |OPTIONS[:environment]| }
    
  opts.on('-w', '--wait-time=seconds', Integer,
    'Specifies the number of seconds to wait between feed refreshes',
    'Default: 300') { |OPTIONS[:wait_time]| }
end

def unescape(text)
  text =~ /&lt;.*&gt;/ ? REXML::Text.unnormalize(text) : text
end

ENV["RAILS_ENV"] = OPTIONS[:environment]
require File.dirname(__FILE__) + '/../config/environment'

quiet = OPTIONS[:quiet]
while (true)
  members = Member.find(:all, "feed_url <> '' and feed_url is not null")
  STDOUT.puts "Beginning feed refresh for #{members.size} Members..." unless quiet
  
  members.each do |member|
    begin
      next unless member.feed_url =~ /\w+/
      
      uri = URI.parse(member.feed_url)
      data = Net::HTTP.get(uri)
      rss = SimpleRSS.parse(data)
      
      puts "Inspecting #{rss.items.size} items for member #{member.email}..." unless quiet
      
      update_count = 0
      new_count = 0
      
      seen = Hash.new
      
      rss.items.each do |item|
        # we care enough about the pubdate to skip items
        # that dont have it
        next if item.pubDate.nil?
				      
        content = unescape(item.summary || item.content || item.description)
	hash = Digest::MD5.hexdigest(content)
	
	# only look at the first one found (there should only be one)
	# since we "index" on the link, if a post is deleted, we would never know...
        found = Article.find(:all, :conditions => ['member_id = ? and link = ?',member.id,item.link]).shift

	# note that we saw this item
	seen[item.link] = 1

	if !found.nil?
	  # no change, nothing to see here
	  next if found.content_hash == hash

	  # update exisiting article
	  found.title = unescape(item.title)
	  found.modified_at = item.pubDate
	  found.content = content
	  found.content_hash = hash
	  found.save
	  
	  update_count += 1
	else
	  # create a new article
	  article = Article.new
          article.member = member
          article.modified_at = (item.pubDate)
          article.title = unescape(item.title)
          article.link = item.link
	  article.content = content
	  article.content_hash = hash.to_s
          article.save

	  new_count += 1
	end

      end

      # delete any we didn't see. this will get rid of old entries and
      # ones that have been deleted. it is also probably not the most
      # efficient way to go about this, but it works for now (famous last words)
      delete_count = 0
      Article.find(:all, :conditions => ['member_id = ?',member.id]).each{ |a|
        unless seen.has_key? a.link
	  a.destroy
	  delete_count += 1
	end
      }

      puts "Updated #{update_count} items, created #{new_count} new items, " +
           "and deleted #{delete_count} items for #{member.email}." unless quiet
    rescue Exception => e
      STDERR.puts("Error loading #{member.feed_url}: '#{e}'")
      next
    end
  end
  
  wait = OPTIONS[:wait_time]
  if wait == 0
    puts "Done." unless quiet
    break
  else
    puts "Done. Sleeping for #{wait} seconds..." unless quiet
    sleep wait
  end
end
